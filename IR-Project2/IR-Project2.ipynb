{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from stopwordsiso import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- preprocessing ----------------------------\n",
    "# normalizer\n",
    "my_normalizer = Normalizer()\n",
    "# tokenizer\n",
    "my_tokenizer = Tokenizer()\n",
    "# stemmer\n",
    "my_stemmer = FindStems()\n",
    "# stop words\n",
    "persian_stopwords = stopwords(\"fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening JSON file\n",
    "# f = open('IR_data_news_small.json')\n",
    "f = open('IR_data_news_12k.json')\n",
    "# returns JSON object as a dictionary\n",
    "documents = json.load(f)\n",
    "N = len(documents)\n",
    "# closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_index = {}\n",
    "\n",
    "list_stem_stopword_token_normal = []\n",
    "\n",
    "# iterating through the json list\n",
    "for docID in documents:\n",
    "    # normalize\n",
    "    normal = my_normalizer.normalize(documents[docID][\"content\"])\n",
    "    # tokenize\n",
    "    token_normal = my_tokenizer.tokenize_words(normal)\n",
    "    # remove stopwords\n",
    "    stopword_token_normal = [t for t in token_normal if t not in persian_stopwords]\n",
    "    # stemming\n",
    "    stem_stopword_token_normal = [my_stemmer.convert_to_stem(w) for w in stopword_token_normal]\n",
    "    list_stem_stopword_token_normal.append(stem_stopword_token_normal)\n",
    "    \n",
    "    # --------------------------- document indexing --------------------------\n",
    "    # creating document index\n",
    "    docLen = len(stem_stopword_token_normal)\n",
    "    for pos in range(docLen):\n",
    "        term = stem_stopword_token_normal[pos]\n",
    "        if term not in document_index: # first visit of this term in all documents\n",
    "            document_index[term] = {docID: {'doc_freq': 1}}\n",
    "        else:\n",
    "            if docID not in document_index[term]: # first visit of this term in this document \n",
    "                document_index[term][docID] = {'doc_freq': 1}\n",
    "            else: # not first visit of this term in this document\n",
    "                document_index[term][docID]['doc_freq'] += 1\n",
    "\n",
    "# compute weight for each term\n",
    "for term in document_index:\n",
    "    n_t = len(document_index[term])\n",
    "    for docID in document_index[term]:\n",
    "        f_t_d = document_index[term][docID]['doc_freq']\n",
    "        document_index[term][docID]['weight'] = ((1 + math.log10(f_t_d)) * math.log10(N/n_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'فوتبال': 0.5662095847744087, 'قهرمانی': 1.0314351591272273, 'آسیا': 1.0644152808386484}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- query processing --------------------------\n",
    "# preprocess query\n",
    "query = input('کوئری خود را وارد کنید: ')\n",
    "query_n = my_normalizer.normalize(query)\n",
    "query_nt = my_tokenizer.tokenize_words(query_n)\n",
    "ps = stopwords(\"fa\")\n",
    "query_ntw = [t for t in query_nt if t not in ps]\n",
    "query_ntws = [my_stemmer.convert_to_stem(w) for w in query_ntw]\n",
    "\n",
    "# query vector\n",
    "query_v = {}\n",
    "for term in query_ntws:\n",
    "    if term not in query_v:\n",
    "        query_v[term] = 1\n",
    "    else:\n",
    "        query_v[term] += 1\n",
    "\n",
    "for term in query_v:\n",
    "    if term not in document_index:\n",
    "        query_v[term] = 0\n",
    "    else:\n",
    "        n_t = len(document_index[term])\n",
    "        query_v[term] = (1 + math.log10(query_v[term])) * math.log10(N/n_t)\n",
    "\n",
    "query_vector = {}\n",
    "for term in query_v:\n",
    "    if query_v[term] != 0:\n",
    "        query_vector[term] = query_v[term]\n",
    "\n",
    "print(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# champion lists\n",
    "rel_docs = {}\n",
    "for term in query_vector:\n",
    "    rel_docs[term] = {}\n",
    "    for docID in document_index[term]:\n",
    "        rel_docs[term][docID] = {}\n",
    "        rel_docs[term][docID]['a_2'] = math.pow(document_index[term][docID]['weight'], 2)\n",
    "        rel_docs[term][docID]['weight'] = document_index[term][docID]['weight'] * query_vector[term]\n",
    "\n",
    "rel_docs_1 = {}\n",
    "for term in rel_docs:\n",
    "    for docID in rel_docs[term]:\n",
    "        if docID not in rel_docs_1:\n",
    "            rel_docs_1[docID] = {}\n",
    "            rel_docs_1[docID]['sum_a_2'] = rel_docs[term][docID]['a_2']\n",
    "            rel_docs_1[docID]['weight'] = rel_docs[term][docID]['weight']\n",
    "        else:\n",
    "            rel_docs_1[docID]['sum_a_2'] += rel_docs[term][docID]['a_2']\n",
    "            rel_docs_1[docID]['weight'] += rel_docs[term][docID]['weight']\n",
    "\n",
    "radical_sum_b_2 = 0\n",
    "for term in query_vector:\n",
    "    radical_sum_b_2 += math.pow(query_vector[term], 2)\n",
    "radical_sum_b_2 = math.sqrt(radical_sum_b_2)\n",
    "\n",
    "relevant_docs = {}\n",
    "for docID in rel_docs_1:\n",
    "    relevant_docs[docID] = (rel_docs_1[docID]['weight'] / (math.sqrt(rel_docs_1[docID]['sum_a_2']) * radical_sum_b_2))\n",
    "\n",
    "relevant_docs_sorted = sorted(relevant_docs.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3177', 1.0), ('3715', 0.9999999999999999), ('6612', 0.9999999999999999), ('84', 0.9999999999999998), ('104', 0.9999999999999998)]\n",
      "title: \n",
      "بر خلاف ادعای صالحی امیری؛ یک فیفادی از دست رفت و خبری از برنامه‌ نیست/ امیدها روی نوار بلاتکلیفی\n",
      "content: \n",
      "خبرنگار ورزشی خبرگزاری فارس ، تیم ملی فوتبال امید کشورمان در خرداد ماه باید \n",
      "کشورمان در خرداد ماه باید در مسابقات قهرمانی آسیا در ازبکستان شرکت کند اما \n",
      "در خرداد ماه باید در مسابقات قهرمانی آسیا در ازبکستان شرکت کند اما با \n",
      "تیم‌ها همگروه نشود . در واقع تیم فوتبال امید باید در مسابقات قهرمانی آسیا \n",
      "واقع تیم فوتبال امید باید در مسابقات قهرمانی آسیا با تیم‌های بسیار قوی چون \n",
      "تیم فوتبال امید باید در مسابقات قهرمانی آسیا با تیم‌های بسیار قوی چون عربستان \n",
      "شد و همین موضوع روند آماده‌سازی تیم فوتبال امید را کندتر می‌کند . آیا \n",
      "مشکلات را کرده است ؟ طبعا مسابقات قهرمانی آسیا با مرحله مقدماتی متفاوت است \n",
      "را کرده است ؟ طبعا مسابقات قهرمانی آسیا با مرحله مقدماتی متفاوت است و \n",
      "کامل وارد مسابقات می‌شوند اما هنوز فدراسیون فوتبال زمانی برای آماده‌سازی و برنامه اردویی \n",
      "فدراسیون داد که زمان کافی برای مسابقات قهرمانی آسیا وجود ندارد و تیم ملی \n",
      "داد که زمان کافی برای مسابقات قهرمانی آسیا وجود ندارد و تیم ملی امید \n",
      "\n",
      "\n",
      "\n",
      "title: \n",
      "آغاز تمرینات تیم فوتبال امید در ابهام/ تکلیف قرارداد کادرفنی مشخص نیست\n",
      "content: \n",
      "ورزشی خبرگزاری فارس ، قرار‌بود تیم ملی فوتبال امید کشورمان همزمان با برگزاری اردوی \n",
      "آماده‌سازی خود را برای حضور در رقابت‌های قهرمانی زیر 23 سال آسیا با حضور \n",
      "حضور در رقابت‌های قهرمانی زیر 23 سال آسیا با حضور نفرات لیگ برتری برگزار \n",
      "تاجیکستان به عنوان صدرنشین مرحله مقدماتی رقابت‌های قهرمانی آسیا جواز حضور در مرحله نهایی \n",
      "به عنوان صدرنشین مرحله مقدماتی رقابت‌های قهرمانی آسیا جواز حضور در مرحله نهایی مسابقات \n",
      "دستیارانش باشد . باید از مدیران فدراسیون فوتبال هم که این روزها تمام تمرکز \n",
      "سال آینده در بازی‌های آسیایی هانگژو و قهرمانی زیر 23 سال آسیا حاضر شود \n",
      "آسیایی هانگژو و قهرمانی زیر 23 سال آسیا حاضر شود مشخص نمی‌کنند تا شاید \n",
      "باشد ؟ مسئولان تیم امید و فدراسیون فوتبال در روزهای اخیر حتی مراحل اخذ \n",
      "\n",
      "\n",
      "\n",
      "title: \n",
      "پیشتازی قهرمان آسیا از چلسی در کسب جایزه بهترین باشگاه دنیا+عکس\n",
      "content: \n",
      "« گلوب ساکر » است تا بهترین‌های فوتبال در سال 2021 را معرفی کند \n",
      "کمپانی اماراتی اهدا می‌کند جایزه بهترین باشگاه فوتبال است . در نظرسنجی بهترین باشگاه \n",
      "بهترین باشگاه سال 2021 جهان الهلال قهرمان آسیا پیشتاز است . تیم سعودی با \n",
      "سال 2021 قرار دارند . الهلال با قهرمانی در لیگ قهرمانان آسیا 2021 با \n",
      ". الهلال با قهرمانی در لیگ قهرمانان آسیا 2021 با 4 قهرمانی پرافتخارترین باشگاه \n",
      "در لیگ قهرمانان آسیا 2021 با 4 قهرمانی پرافتخارترین باشگاه قاره کهن شد . \n",
      "\n",
      "\n",
      "\n",
      "title: \n",
      "تمجید ویژه رئیس فدراسیون عراق از ناظم الشریعه\n",
      "content: \n",
      "حال آماده‌سازی برای حضور در مسابقات مقدماتی قهرمانی آسیا هستند . در همین رابطه \n",
      "آماده‌سازی برای حضور در مسابقات مقدماتی قهرمانی آسیا هستند . در همین رابطه عدنان \n",
      "در همین رابطه عدنان درجال رئیس فدراسیون فوتبال این کشور در اردوی تیم ملی \n",
      "\n",
      "\n",
      "\n",
      "title: \n",
      "محمدخانی: پرسپولیس باید مقابل استقلال انتحاری بازی کند\n",
      "content: \n",
      "بازی مساوی است با از دست رفتن قهرمانی لیگ ، چون اختلاف امتیازی با \n",
      "دقایق و نتایج این مسابقه در تاریخ فوتبال ثبت می‌شود . هنوز در مورد \n",
      "می‌آید . حامد لک در لیگ قهرمانان آسیا هم برای پرسپولیس خوب دروازه‌بانی کرد \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_relevant_contents(docID, terms):\n",
    "    doc = documents[str(docID)][\"content\"]\n",
    "    doc_n = my_normalizer.normalize(doc)\n",
    "    doc_nt = my_tokenizer.tokenize_words(doc_n)\n",
    "    doc_nts = [my_stemmer.convert_to_stem(w) for w in doc_nt]\n",
    "\n",
    "    for i in range(len(doc_nts)):\n",
    "        term = doc_nts[i]\n",
    "        if term in terms:\n",
    "            for t in doc_nt[i-7: i+7]:\n",
    "                print(t, end=\" \")\n",
    "            print(end=\"\\n\")\n",
    "\n",
    "print(relevant_docs_sorted[:5])\n",
    "k = 0\n",
    "for doc in relevant_docs_sorted:\n",
    "    if k >= 5:\n",
    "        break\n",
    "    k += 1\n",
    "    print('title: ')\n",
    "    print(documents[doc[0]]['title'], end='\\n')\n",
    "    print('content: ')\n",
    "    print_relevant_contents(doc[0], query_vector.keys())\n",
    "    print('\\n\\n')    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cacd30a67e9c54f76d0487f5b076fd54b14c281e005077295ff3a8a79ca91f14"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
