{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from parsivar import Normalizer, Tokenizer, FindStems\n",
    "from stopwordsiso import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- preprocessing ----------------------------\n",
    "# normalizer\n",
    "my_normalizer = Normalizer()\n",
    "# tokenizer\n",
    "my_tokenizer = Tokenizer()\n",
    "# stemmer\n",
    "my_stemmer = FindStems()\n",
    "# stop words\n",
    "persian_stopwords = stopwords(\"fa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening JSON file\n",
    "f = open('IR_data_news_small.json')\n",
    "# returns JSON object as a dictionary\n",
    "documents = json.load(f)\n",
    "# closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_index = {}\n",
    "\n",
    "# iterating through the json list\n",
    "for docID in documents:\n",
    "    # normalize\n",
    "    normal = my_normalizer.normalize(documents[docID][\"content\"])\n",
    "    # tokenize\n",
    "    token_normal = my_tokenizer.tokenize_words(normal)\n",
    "    # remove stopwords\n",
    "    stopword_token_normal = []\n",
    "    for t in token_normal:\n",
    "        if t not in persian_stopwords:\n",
    "            stopword_token_normal.append(t)\n",
    "    # stemming\n",
    "    stem_stopword_token_normal = [my_stemmer.convert_to_stem(w) for w in stopword_token_normal]\n",
    "\n",
    "    # --------------------------- positional index --------------------------\n",
    "    # creating positional index\n",
    "    docLen = len(stem_stopword_token_normal)\n",
    "    for pos in range(docLen):\n",
    "        term = stem_stopword_token_normal[pos]\n",
    "        if term not in positional_index: # first visit of this term in all documents\n",
    "            positional_index[term] = {'tot_freq': 1, docID: {'doc_freq': 1, 'positions': [pos]}}\n",
    "        else:\n",
    "            positional_index[term]['tot_freq'] += 1\n",
    "            if docID not in positional_index[term]: # first visit of this term in this document \n",
    "                positional_index[term][docID] = {'doc_freq': 1, 'positions': [pos]}\n",
    "            else: # not first visit of this term in this document\n",
    "                positional_index[term][docID]['doc_freq'] += 1\n",
    "                positional_index[term][docID]['positions'].append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "کوئری خود را وارد کنید: کشور\n",
      "کشور\n"
     ]
    }
   ],
   "source": [
    "query = input('کوئری خود را وارد کنید: ')\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
